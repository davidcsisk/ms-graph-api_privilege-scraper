{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c2de60f",
   "metadata": {},
   "source": [
    "#### Explore Graph API Permission Knowledge Trained into CodeLlama LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b40c9687",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_URL = \"http://ollamaserver_nuc1:11434/api/generate\"\n",
    "MODEL_NAME = \"codellama:34b\"\n",
    "#MODEL_NAME = \"codellama:7b\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4c6e062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"model\": \"codellama:34b\",\n",
      "  \"created_at\": \"2025-12-18T17:07:35.916503184Z\",\n",
      "  \"response\": \"\",\n",
      "  \"done\": true,\n",
      "  \"done_reason\": \"load\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Ask the CodeLlama model to identify itself, its expertise, and its training-data cutoff\n",
    "user_content = (\n",
    "    \"Identify yourself briefly. Return a short JSON object with fields: \"\n",
    "    \"'model' (string), 'expertise' (list of short strings), and 'training_data_cutoff' (ISO date or year). \"\n",
    "    \"Keep the response concise.\"\n",
    ")\n",
    "\n",
    "identify_payload = {\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ],\n",
    "    \"temperature\": temperature,\n",
    "    \"max_tokens\": 200,\n",
    "}\n",
    "\n",
    "resp = requests.post(OLLAMA_URL, json=identify_payload, timeout=60)\n",
    "resp.raise_for_status()\n",
    "identify_result = resp.json()\n",
    "\n",
    "identify_reply = None\n",
    "if \"choices\" in identify_result and identify_result[\"choices\"]:\n",
    "    identify_reply = identify_result[\"choices\"][0].get(\"message\", {}).get(\"content\")\n",
    "elif \"results\" in identify_result and identify_result[\"results\"]:\n",
    "    identify_reply = identify_result[\"results\"][0].get(\"content\")\n",
    "\n",
    "if identify_reply:\n",
    "    print(identify_reply)\n",
    "else:\n",
    "    print(json.dumps(identify_result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a3f948b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"model\": \"codellama:34b\",\n",
      "  \"created_at\": \"2025-12-18T17:07:47.395186015Z\",\n",
      "  \"response\": \"\",\n",
      "  \"done\": true,\n",
      "  \"done_reason\": \"load\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Example: set system prompt and temperature when calling the CodeLlama model\n",
    "system_prompt = \"You are a concise assistant that returns short, precise code examples.\"\n",
    "temperature = 0.1  # lower = more deterministic, higher = more creative\n",
    "\n",
    "payload = {\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": \"Provide a one-line Python function that reverses a string.\"}\n",
    "    ],\n",
    "    \"temperature\": temperature,\n",
    "    \"max_tokens\": 150,\n",
    "}\n",
    "\n",
    "resp = requests.post(OLLAMA_URL, json=payload, timeout=60)\n",
    "resp.raise_for_status()\n",
    "result = resp.json()\n",
    "\n",
    "# Extract and print the model's reply (response structure may vary by server)\n",
    "reply = None\n",
    "if \"choices\" in result and result[\"choices\"]:\n",
    "    reply = result[\"choices\"][0].get(\"message\", {}).get(\"content\")\n",
    "elif \"results\" in result and result[\"results\"]:\n",
    "    # Ollama sometimes returns 'results'\n",
    "    reply = result[\"results\"][0].get(\"content\")\n",
    "\n",
    "if reply:\n",
    "    print(reply)\n",
    "else:\n",
    "    print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8375934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llama(question: str, echo: bool = True, raise_on_error: bool = False, **ask_kwargs):\n",
    "    \"\"\"\n",
    "    Ask the CodeLlama model a question and return (reply, raw_result).\n",
    "\n",
    "    Parameters:\n",
    "      - question: the user question to send to the model\n",
    "      - echo: if True, print the reply (or raw response if no text)\n",
    "      - raise_on_error: if True, re-raise exceptions instead of printing them\n",
    "      - **ask_kwargs: forwarded to ask_model (e.g., system_prompt, temperature, max_tokens, timeout)\n",
    "\n",
    "    Returns:\n",
    "      (reply: Optional[str], raw_result: dict or None)\n",
    "    \"\"\"\n",
    "    if not question or not isinstance(question, str):\n",
    "        raise ValueError(\"question must be a non-empty string\")\n",
    "\n",
    "    try:\n",
    "        reply, raw = ask_model(question, **ask_kwargs)\n",
    "    except Exception as e:\n",
    "        if raise_on_error:\n",
    "            raise\n",
    "        print(f\"Error calling model: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    if echo:\n",
    "        if reply:\n",
    "            print(reply)\n",
    "        else:\n",
    "            print(\"Model returned no text. Raw response:\")\n",
    "            print(json.dumps(raw, indent=2))\n",
    "\n",
    "    return reply, raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ecee6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model returned no textual reply. Raw response:\n",
      "{\n",
      "  \"model\": \"codellama:34b\",\n",
      "  \"created_at\": \"2025-12-18T17:41:52.030943856Z\",\n",
      "  \"response\": \"\",\n",
      "  \"done\": true,\n",
      "  \"done_reason\": \"load\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Call ask_llama and print its reply (suppress echo inside the function so we print consistently)\n",
    "question = \"Provide a one-line Python function that reverses a string.\"\n",
    "reply, raw = ask_llama(question, echo=False)\n",
    "\n",
    "if reply:\n",
    "    print(\"Model reply:\\n\", reply)\n",
    "else:\n",
    "    print(\"Model returned no textual reply. Raw response:\")\n",
    "    print(json.dumps(raw, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bae415d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_model(user_text, model=MODEL_NAME, system_prompt=None, temperature=0.1, max_tokens=512, timeout=60):\n",
    "    \"\"\"Send a request to the Ollama generate endpoint and return (reply_text, raw_json).\n",
    "\n",
    "    Uses the chat-style \"messages\" payload when a system prompt is provided.\n",
    "    \"\"\"\n",
    "    import requests, json\n",
    "\n",
    "    if system_prompt is None:\n",
    "        system_prompt = \"\"\n",
    "\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_text},\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "\n",
    "    resp = requests.post(OLLAMA_URL, json=payload, timeout=timeout)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "\n",
    "    # Normalize reply extraction across possible response formats\n",
    "    reply = None\n",
    "    if \"choices\" in data and data[\"choices\"]:\n",
    "        reply = data[\"choices\"][0].get(\"message\", {}).get(\"content\")\n",
    "    elif \"results\" in data and data[\"results\"]:\n",
    "        # Ollama sometimes returns a results list with dicts\n",
    "        first = data[\"results\"][0]\n",
    "        if isinstance(first, dict):\n",
    "            reply = first.get(\"content\")\n",
    "        else:\n",
    "            reply = first\n",
    "    else:\n",
    "        # older scripts used a top-level 'response' key\n",
    "        reply = data.get(\"response\")\n",
    "\n",
    "    # Ensure reply is a stripped string if present\n",
    "    if isinstance(reply, str):\n",
    "        reply = reply.strip()\n",
    "\n",
    "    return reply, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fdf1f026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No textual reply. Raw response:\n",
      "{\n",
      "  \"model\": \"codellama:34b\",\n",
      "  \"created_at\": \"2025-12-18T17:41:07.842901317Z\",\n",
      "  \"response\": \"\",\n",
      "  \"done\": true,\n",
      "  \"done_reason\": \"load\"\n",
      "}\n",
      "\n",
      "Reply2: (no text)\n",
      "Raw2 done_reason: load\n",
      "\n",
      "Reply3: (no text)\n"
     ]
    }
   ],
   "source": [
    "# Examples showing how to call ask_model and handle responses\n",
    "\n",
    "# 1) Basic call (uses MODEL_NAME and defaults)\n",
    "prompt = \"Provide a one-line Python function that reverses a string.\"\n",
    "reply, raw = ask_model(prompt)\n",
    "if reply:\n",
    "    print(\"Reply:\\n\", reply)\n",
    "else:\n",
    "    print(\"No textual reply. Raw response:\")\n",
    "    print(json.dumps(raw, indent=2))\n",
    "\n",
    "# 2) With a custom system prompt, temperature, and max_tokens\n",
    "reply2, raw2 = ask_model(\n",
    "    \"Give 3 concise tips for debugging Python code.\",\n",
    "    system_prompt=\"You are concise and practical.\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=120,\n",
    ")\n",
    "print(\"\\nReply2:\", reply2 or \"(no text)\")\n",
    "print(\"Raw2 done_reason:\", raw2.get(\"done_reason\"))\n",
    "\n",
    "# 3) Override model, increase timeout, and catch exceptions\n",
    "try:\n",
    "    reply3, raw3 = ask_model(\n",
    "        \"Summarize the benefits of unit tests in 2 sentences.\",\n",
    "        model=\"codellama:34b\",\n",
    "        timeout=30,\n",
    "    )\n",
    "    print(\"\\nReply3:\", reply3 or \"(no text)\")\n",
    "except Exception as e:\n",
    "    print(\"ask_model raised an exception:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
